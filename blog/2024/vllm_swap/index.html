<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Swap policy in vllm | Yuyi Ao </title> <meta name="author" content="Yuyi Ao"> <meta name="description" content="This post is to discuss the swap policy in vllm."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://george-ao.github.io/blog/2024/vllm_swap/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?bf50d6d9dd867d3e0f3b0add94449649"></script> </head> <body class=" "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm sticky-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Yuyi </span> Ao </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Miscellaneous </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Swap policy in vllm</h1> <p class="post-meta"> April 17, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/ml"> <i class="fa-solid fa-hashtag fa-sm"></i> ml</a>     ·   <a href="/blog/category/sample-posts"> <i class="fa-solid fa-tag fa-sm"></i> sample-posts</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <p>This post is to discuss the swap policy in vllm. In each step of <code class="language-plaintext highlighter-rouge">llm.engine</code>, the <code class="language-plaintext highlighter-rouge">scheduler</code> schedules the blocks to be swapped in and out. Then, the relevant information about the swap-in and swap-out blocks is passed to the <code class="language-plaintext highlighter-rouge">model_executor</code> through <code class="language-plaintext highlighter-rouge">scheduler_outputs</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">RequestOutput</span><span class="p">]:</span>
        <span class="n">seq_group_metadata_list</span><span class="p">,</span> <span class="n">scheduler_outputs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">scheduler</span><span class="p">.</span><span class="nf">schedule</span><span class="p">()</span>
        <span class="c1"># pdb.set_trace()
</span>        <span class="k">if</span> <span class="ow">not</span> <span class="n">scheduler_outputs</span><span class="p">.</span><span class="nf">is_empty</span><span class="p">():</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">model_executor</span><span class="p">.</span><span class="nf">execute_model</span><span class="p">(</span>
                <span class="n">seq_group_metadata_list</span><span class="p">,</span> <span class="n">scheduler_outputs</span><span class="p">.</span><span class="n">blocks_to_swap_in</span><span class="p">,</span>
                <span class="n">scheduler_outputs</span><span class="p">.</span><span class="n">blocks_to_swap_out</span><span class="p">,</span>
                <span class="n">scheduler_outputs</span><span class="p">.</span><span class="n">blocks_to_copy</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">_process_model_outputs</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">scheduler_outputs</span><span class="p">)</span>
</code></pre></div></div> <p>The swap-in and swap-out blocks are decided in <code class="language-plaintext highlighter-rouge">scheduler.schedule()</code> function, which calls <code class="language-plaintext highlighter-rouge">scheduler._schedule()</code>. The scheduler will try to swap in the blocks from cpu to gpu if it is possible. The scheduler will also swap out the blocks from gpu to cpu if <code class="language-plaintext highlighter-rouge">block_manager.can_append_slot</code> fails. In that case, the scheduler will preempt lower priority seq groups to make space for the higher priority seq groups. In vllm, there are two ways to preempt low priority sequence group: <code class="language-plaintext highlighter-rouge">recompute</code> and <code class="language-plaintext highlighter-rouge">swap</code>. According to the code, the current policy is <code class="language-plaintext highlighter-rouge">recompute</code> only when seq_group.get_max_num_running_seqs() == 1. <code class="language-plaintext highlighter-rouge">scheduler.schedule()</code> will return <code class="language-plaintext highlighter-rouge">scheduler_outputs</code> which contains necessary information for <code class="language-plaintext highlighter-rouge">model_executor</code> to do the swap-in and swap-out. Following are the code showcasing what functions model_executor will call:</p> <ol> <li> <p>In <code class="language-plaintext highlighter-rouge">model_executor.execute_model()</code>, it will call <code class="language-plaintext highlighter-rouge">driver_worker</code> to do the job.</p> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>   <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">driver_worker</span><span class="p">.</span><span class="nf">execute_model</span><span class="p">(</span>
   <span class="n">seq_group_metadata_list</span><span class="o">=</span><span class="n">seq_group_metadata_list</span><span class="p">,</span>
   <span class="n">blocks_to_swap_in</span><span class="o">=</span><span class="n">blocks_to_swap_in</span><span class="p">,</span>
   <span class="n">blocks_to_swap_out</span><span class="o">=</span><span class="n">blocks_to_swap_out</span><span class="p">,</span>
   <span class="n">blocks_to_copy</span><span class="o">=</span><span class="n">blocks_to_copy</span><span class="p">,</span>
 <span class="p">)</span>
</code></pre></div> </div> </li> <li> <p>In <code class="language-plaintext highlighter-rouge">worker</code>, it will call <code class="language-plaintext highlighter-rouge">cache_swap</code> (some codes are ommited)</p> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>   <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">is_driver_worker</span><span class="p">:</span>
   <span class="k">assert</span> <span class="n">seq_group_metadata_list</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span>
   <span class="n">num_seq_groups</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">seq_group_metadata_list</span><span class="p">)</span>
   <span class="k">assert</span> <span class="n">blocks_to_swap_in</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span>
   <span class="k">assert</span> <span class="n">blocks_to_swap_out</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span>
   <span class="k">assert</span> <span class="n">blocks_to_copy</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span>
   <span class="n">data</span> <span class="o">=</span> <span class="p">{</span>
       <span class="sh">"</span><span class="s">num_seq_groups</span><span class="sh">"</span><span class="p">:</span> <span class="n">num_seq_groups</span><span class="p">,</span>
       <span class="sh">"</span><span class="s">blocks_to_swap_in</span><span class="sh">"</span><span class="p">:</span> <span class="n">blocks_to_swap_in</span><span class="p">,</span>
       <span class="sh">"</span><span class="s">blocks_to_swap_out</span><span class="sh">"</span><span class="p">:</span> <span class="n">blocks_to_swap_out</span><span class="p">,</span>
       <span class="sh">"</span><span class="s">blocks_to_copy</span><span class="sh">"</span><span class="p">:</span> <span class="n">blocks_to_copy</span><span class="p">,</span>
   <span class="p">}</span>
   <span class="nf">broadcast_tensor_dict</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
   <span class="n">self</span><span class="p">.</span><span class="nf">cache_swap</span><span class="p">(</span><span class="n">blocks_to_swap_in</span><span class="p">,</span> <span class="n">blocks_to_swap_out</span><span class="p">,</span> <span class="n">blocks_to_copy</span><span class="p">)</span>
</code></pre></div> </div> </li> <li> <p>Then, it will call <code class="language-plaintext highlighter-rouge">cache_engine</code> to swap the blocks.</p> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>     <span class="k">def</span> <span class="nf">swap_in</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">src_to_dst</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
         <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">num_layers</span><span class="p">):</span>
             <span class="n">self</span><span class="p">.</span><span class="n">attn_backend</span><span class="p">.</span><span class="nf">swap_blocks</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">cpu_cache</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">self</span><span class="p">.</span><span class="n">gpu_cache</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                                         <span class="n">src_to_dst</span><span class="p">)</span>
</code></pre></div> </div> </li> <li>In <code class="language-plaintext highlighter-rouge">attn_backend</code>(xformer in my case), it will call <code class="language-plaintext highlighter-rouge">Pageattention.swap_blocks</code> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>     <span class="k">def</span> <span class="nf">swap_blocks</span><span class="p">(</span>
         <span class="n">src_kv_cache</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span>
         <span class="n">dst_kv_cache</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span>
         <span class="n">src_to_dst</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
     <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
         <span class="n">PagedAttention</span><span class="p">.</span><span class="nf">swap_blocks</span><span class="p">(</span><span class="n">src_kv_cache</span><span class="p">,</span> <span class="n">dst_kv_cache</span><span class="p">,</span> <span class="n">src_to_dst</span><span class="p">)</span>
</code></pre></div> </div> </li> <li>And then it will use <code class="language-plaintext highlighter-rouge">cache_ops</code>.(some codes are ommited) <div class="language-c++ highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>     <span class="kt">void</span> <span class="nf">swap_blocks</span><span class="p">(</span>
         <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&amp;</span> <span class="n">src</span><span class="p">,</span>
         <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&amp;</span> <span class="n">dst</span><span class="p">,</span>
         <span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">map</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="p">,</span> <span class="kt">int64_t</span><span class="o">&gt;&amp;</span> <span class="n">block_mapping</span><span class="p">)</span> <span class="p">{</span>
         <span class="kt">char</span> <span class="o">*</span><span class="n">src_ptr</span> <span class="o">=</span> <span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">char</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">src</span><span class="p">.</span><span class="n">data_ptr</span><span class="p">());</span>
         <span class="kt">char</span> <span class="o">*</span><span class="n">dst_ptr</span> <span class="o">=</span> <span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">char</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">dst</span><span class="p">.</span><span class="n">data_ptr</span><span class="p">());</span>
         <span class="k">const</span> <span class="kt">int64_t</span> <span class="n">block_size_in_bytes</span> <span class="o">=</span> <span class="n">src</span><span class="p">.</span><span class="n">element_size</span><span class="p">()</span> <span class="o">*</span> <span class="n">src</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">numel</span><span class="p">();</span>
         <span class="k">const</span> <span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">OptionalCUDAGuard</span> <span class="n">device_guard</span><span class="p">(</span><span class="n">src_device</span><span class="p">.</span><span class="n">is_cuda</span><span class="p">()</span> <span class="o">?</span> <span class="n">src_device</span> <span class="o">:</span> <span class="n">dst_device</span><span class="p">);</span>
         <span class="k">const</span> <span class="n">cudaStream_t</span> <span class="n">stream</span> <span class="o">=</span> <span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">getCurrentCUDAStream</span><span class="p">();</span>
         <span class="c1">// NOTE(woosuk): This can be slow if the number of blocks is large.</span>
         <span class="k">for</span> <span class="p">(</span><span class="k">const</span> <span class="k">auto</span><span class="o">&amp;</span> <span class="n">pair</span> <span class="o">:</span> <span class="n">block_mapping</span><span class="p">)</span> <span class="p">{</span>
             <span class="kt">int64_t</span> <span class="n">src_block_number</span> <span class="o">=</span> <span class="n">pair</span><span class="p">.</span><span class="n">first</span><span class="p">;</span>
             <span class="kt">int64_t</span> <span class="n">dst_block_number</span> <span class="o">=</span> <span class="n">pair</span><span class="p">.</span><span class="n">second</span><span class="p">;</span>
             <span class="kt">int64_t</span> <span class="n">src_offset</span> <span class="o">=</span> <span class="n">src_block_number</span> <span class="o">*</span> <span class="n">block_size_in_bytes</span><span class="p">;</span>
             <span class="kt">int64_t</span> <span class="n">dst_offset</span> <span class="o">=</span> <span class="n">dst_block_number</span> <span class="o">*</span> <span class="n">block_size_in_bytes</span><span class="p">;</span>
             <span class="n">cudaMemcpyAsync</span><span class="p">(</span>
             <span class="n">dst_ptr</span> <span class="o">+</span> <span class="n">dst_offset</span><span class="p">,</span>
             <span class="n">src_ptr</span> <span class="o">+</span> <span class="n">src_offset</span><span class="p">,</span>
             <span class="n">block_size_in_bytes</span><span class="p">,</span>
             <span class="n">memcpy_type</span><span class="p">,</span>
             <span class="n">stream</span><span class="p">);</span>
         <span class="p">}</span>
     <span class="p">}</span>
</code></pre></div> </div> <p>The graph might be useful to understand the vllm structure.</p> </li> </ol> <swiper-container keyboard="true" navigation="true" pagination="true" pagination-clickable="true" pagination-dynamic-bullets="true" rewind="true"> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/vllm_structure-480.webp 480w,/assets/img/vllm_structure-800.webp 800w,/assets/img/vllm_structure-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/vllm_structure.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </swiper-slide> </swiper-container> <p>References: <a href="https://zhuanlan.zhihu.com/p/645251151" rel="external nofollow noopener" target="_blank">vLL framework</a>; <a href="https://github.com/vllm-project/vllm" rel="external nofollow noopener" target="_blank">vllm</a></p> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Yuyi Ao. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: April 19, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script type="text/javascript">$(function(){$('[data-toggle="tooltip"]').tooltip()});</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?7254ae07fe9cc5f3a10843e1c0817c9c" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script>var wechatModal=document.getElementById("WeChatMod"),wechatBtn=document.getElementById("WeChatBtn");wechatBtn.onclick=function(){wechatModal.style.display="block"},window.onclick=function(t){t.target==wechatModal&&(wechatModal.style.display="none")};</script> </body> </html>