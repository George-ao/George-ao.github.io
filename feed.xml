<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://george-ao.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://george-ao.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-01-31T02:31:03+00:00</updated><id>https://george-ao.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">test</title><link href="https://george-ao.github.io/blog/2024/ml/" rel="alternate" type="text/html" title="test"/><published>2024-01-30T00:00:00+00:00</published><updated>2024-01-30T00:00:00+00:00</updated><id>https://george-ao.github.io/blog/2024/ml</id><content type="html" xml:base="https://george-ao.github.io/blog/2024/ml/"><![CDATA[<h4 id="credits-this-post-records-my-learning-of-uiuc-cs446-taught-by-prof-han-zhao-and-shenlong-wang">Credits: This post records my learning of UIUC CS446, taught by Prof. Han Zhao and Shenlong Wang.</h4> <h2 id="linear-regression">Linear Regression</h2> <h3 id="simple-case"><em>Simple Case</em></h3> <p>( a^2 + b^2 = c^2 ) \( {a^2 + b^2 = c^2} \) \( {a^2 + b^2 = c^2} \) Linear regression is a simple method to solve a regression problem. It has a <strong>closed form</strong> solution. Let me first illustrate how to get the closed form solution. Suppose we have a dataset \( {(x^1,y^1),…,(x^n,y^n)} \), where \(x^i\in\mathbb{R}^d\) and \(y^i\in\mathbb{R}\). We want to find a linear function \(f(x)=w^Tx+b\) to fit the data.</p> <p>To make the question simple, we first discuss the case that d=1, i.e., \(x^i\) is a real number. In this case, we can write the linear function as \(f(x)=w_1x+w_2\). Therefore, what we want to find is the optimal \(w_1\) and \(w_2\) such that \(\underset{w_1,w_2}{\operatorname{argmin}} \frac{1}{2}\sum_{i=1}^n(y^i-w_1x^i-w_2)^2\). Translate the problem into matrix form, we have</p> \[\underset{w_1,w_2}{\operatorname{argmin}} \frac{1}{2} \left\| \begin{bmatrix} y^{1} \\ \vdots \\ y^{n} \end{bmatrix} - \begin{bmatrix} x^{1} &amp; 1 \\ \vdots &amp; \vdots \\ x^{n} &amp; 1 \end{bmatrix} \cdot \begin{bmatrix} w_1 \\ w_2 \end{bmatrix} \right\|^2_2\] <p>Then, we just denote the matrix above as \(X^T, Y, w\) respectively. X^T \(\in \mathbb{R}^{n\times 2}\), Y \(\in \mathbb{R}^{n}\), w \(\in \mathbb{R}^{2}\). The problem becomes \(\underset{w}{\operatorname{argmin}} \frac{1}{2}|Y-Xw|^2_2\). To solve this problem, we just need to take the derivative of the objective function with respect to w and set it to 0. With knowledge of matrix calculus, we have \(L = \frac{1}{2}(Y-X^Tw)^T(Y-X^Tw)= \frac{1}{2}(Y^TY-Y^TX^Tw-w^TXY +w^TX^TXw)\\\) \(\frac{\partial L}{\partial w} = \frac{1}{2}(0-XY-XY+2X^TXw) = 0 \\\) \(w = (X^TX)^{-1}XY\)</p> <h3 id="general-case"><em>General Case</em></h3> <p>Let’s go bakc to general case. We also talk about higher order polynomial regression and $x^i$ is no longer a real number. Suppose we have a dataset \({(x^1,y^1),…,(x^n,y^n)}\), where \(x^i\in\mathbb{R}^d\) and \(y^i\in\mathbb{R}\). We want to find a polynomial function \(f(x)=w_0+w_1x+w_2x^2+…+w_dx^d\) to fit the data. \(\underset{w_0, w_1, \ldots, w_d}{\operatorname{argmin}} \frac{1}{2} \left\| \begin{bmatrix} y^{(1)} \\ \vdots \\ y^{(N)} \end{bmatrix} - \begin{bmatrix} (x^{(1)})^d &amp; \cdots &amp; x^{(1)} &amp; 1 \\ \vdots &amp; \ddots &amp; \vdots &amp; \vdots \\ (x^{(N)})^d &amp; \cdots &amp; x^{(N)} &amp; 1 \end{bmatrix} \cdot \begin{bmatrix} w_d \\ \vdots \\ w_1 \\ w_0 \end{bmatrix} \right\|^2\) Then, X^T \(\in \mathbb{R}^{n\times d}\), Y \(\in \mathbb{R}^{n}\), w \(\in \mathbb{R}^{d}\). The problem is also \(\underset{w}{\operatorname{argmin}} \frac{1}{2}|Y-Xw|^2_2\). Similarly, we can take the derivative of the objective function with respect to w and set it to 0. We get the same closed form solution as above. \(w = (XX^T)^{-1}XY\)</p> <h3 id="regularization"><em>Regularization</em></h3> <p>In practice, we may encounter the problem that $n&lt;d+1$. In this case, the matrix $XX^T$ is not invertible. To solve this problem, we can add a regularization term to the objective function. The objective function becomes \(\underset{w}{\operatorname{argmin}} \frac{1}{2}|Y-Xw|^2_2+\frac{\lambda}{2}|w|^2_2\). The closed form solution becomes \(w = (XX^T+\lambda I)^{-1}XY\). Also, regularization can make the parameters smaller and avoid overfitting.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="ml"/><summary type="html"><![CDATA[This post record my learning of linear regression and logistic regression.]]></summary></entry><entry><title type="html">linear regression</title><link href="https://george-ao.github.io/blog/2024/ml/" rel="alternate" type="text/html" title="linear regression"/><published>2024-01-29T00:00:00+00:00</published><updated>2024-01-29T00:00:00+00:00</updated><id>https://george-ao.github.io/blog/2024/ml</id><content type="html" xml:base="https://george-ao.github.io/blog/2024/ml/"><![CDATA[<h4 id="credits-this-post-records-my-learning-of-uiuc-cs446-taught-by-prof-han-zhao-and-shenlong-wang">Credits: This post records my learning of UIUC CS446, taught by Prof. Han Zhao and Shenlong Wang.</h4> <h2 id="linear-regression">Linear Regression</h2> <h3 id="simple-case"><em>Simple Case</em></h3> <p>Linear regression is a simple method to solve a regression problem. It has a <strong>closed form</strong> solution. Let me first illustrate how to get the closed form solution. Suppose we have a dataset ${(x^1,y^1),…,(x^n,y^n)}$, where $x^i\in\mathbb{R}^d$ and $y^i\in\mathbb{R}$. We want to find a linear function $f(x)=w^Tx+b$ to fit the data.</p> <p>To make the question simple, we first discuss the case that d=1, i.e., $x^i$ is a real number. In this case, we can write the linear function as $f(x)=w_1x+w_2$. Therefore, what we want to find is the optimal $w_1$ and $w_2$ such that $\underset{w_1,w_2}{\operatorname{argmin}} \frac{1}{2}\sum_{i=1}^n(y^i-w_1x^i-w_2)^2$. Translate the problem into matrix form, we have</p> \[\underset{w_1,w_2}{\operatorname{argmin}} \frac{1}{2} \left\| \begin{bmatrix} y^{1} \\ \vdots \\ y^{n} \end{bmatrix} - \begin{bmatrix} x^{1} &amp; 1 \\ \vdots &amp; \vdots \\ x^{n} &amp; 1 \end{bmatrix} \cdot \begin{bmatrix} w_1 \\ w_2 \end{bmatrix} \right\|^2_2\] <p>Then, we just denote the matrix above as $X^T, Y, w$ respectively. X^T $\in \mathbb{R}^{n\times 2}$, Y $\in \mathbb{R}^{n}$, w $\in \mathbb{R}^{2}$. The problem becomes $\underset{w}{\operatorname{argmin}} \frac{1}{2}|Y-Xw|^2_2$. To solve this problem, we just need to take the derivative of the objective function with respect to w and set it to 0. With knowledge of matrix calculus, we have \(L = \frac{1}{2}(Y-X^Tw)^T(Y-X^Tw)= \frac{1}{2}(Y^TY-Y^TX^Tw-w^TXY +w^TX^TXw)\\ \frac{\partial L}{\partial w} = \frac{1}{2}(0-XY-XY+2X^TXw) = 0 \\ w = (X^TX)^{-1}XY\)</p> <h3 id="general-case"><em>General Case</em></h3> <p>Let’s go bakc to general case. We also talk about higher order polynomial regression and $x^i$ is no longer a real number. Suppose we have a dataset ${(x^1,y^1),…,(x^n,y^n)}$, where $x^i\in\mathbb{R}^d$ and $y^i\in\mathbb{R}$. We want to find a polynomial function $f(x)=w_0+w_1x+w_2x^2+…+w_dx^d$ to fit the data. \(\underset{w_0, w_1, \ldots, w_d}{\operatorname{argmin}} \frac{1}{2} \left\| \begin{bmatrix} y^{(1)} \\ \vdots \\ y^{(N)} \end{bmatrix} - \begin{bmatrix} (x^{(1)})^d &amp; \cdots &amp; x^{(1)} &amp; 1 \\ \vdots &amp; \ddots &amp; \vdots &amp; \vdots \\ (x^{(N)})^d &amp; \cdots &amp; x^{(N)} &amp; 1 \end{bmatrix} \cdot \begin{bmatrix} w_d \\ \vdots \\ w_1 \\ w_0 \end{bmatrix} \right\|^2\) Then, X^T $\in \mathbb{R}^{n\times d}$, Y $\in \mathbb{R}^{n}$, w $\in \mathbb{R}^{d}$. The problem is also $\underset{w}{\operatorname{argmin}} \frac{1}{2}|Y-Xw|^2_2$. Similarly, we can take the derivative of the objective function with respect to w and set it to 0. We get the same closed form solution as above. \(w = (XX^T)^{-1}XY\)</p> <h3 id="regularization"><em>Regularization</em></h3> <p>In practice, we may encounter the problem that $n&lt;d+1$. In this case, the matrix $XX^T$ is not invertible. To solve this problem, we can add a regularization term to the objective function. The objective function becomes $\underset{w}{\operatorname{argmin}} \frac{1}{2}|Y-Xw|^2_2+\frac{\lambda}{2}|w|^2_2$. The closed form solution becomes $w = (XX^T+\lambda I)^{-1}XY$. Also, regularization can make the parameters smaller and avoid overfitting.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="ml"/><summary type="html"><![CDATA[This post record my learning of linear regression and logistic regression.]]></summary></entry><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://george-ao.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog"/><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://george-ao.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://george-ao.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[]]></content><author><name></name></author></entry></feed>